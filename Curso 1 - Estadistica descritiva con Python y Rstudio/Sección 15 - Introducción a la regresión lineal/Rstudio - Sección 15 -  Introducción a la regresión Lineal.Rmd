---
title: "Rstudio-Sección15-Introducción a la Regresión Lineal"
author: "Arturo Castro"
date: "24/04/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# La regresión lineal

Seguramente, en algún momento de vuestra vida ya se hojeando un libro de matemáticas, curioseando articulos cientificos... habréis visto una línea recta o algún otro tipo de curva en un gráfico que se ajusta a las observaciones representadas por medio de puntos en el plano.

En general, la situación es la siguiente: supongamos que tenemos una serie de puntos en el plano cartesiano $R^2$, de la forma

$$(x_1,y_1),...,(x_n,y_n)$$
que representan las observaciones de dos variables numéricas. Digamos que x es la edad y el peso n estudiantes.

*Nuestro objetivo:* Describir la relación entre la variable independiente, `x`, y la variable dependiente, `y`, a partir de estas observaciones.

Para ello, lo que haremos sera buscar una función `y-f(x)` cuya gráfica se aproxime lo máximo posible a nuestros pares ordenados $(x_i,y_i)_{i=1,...,n}$.

Esta función nos dará un modelo matemático de como se comportan estas observaciones, lo cual nos permitirá entender mejor los mecanismos que relacionan las variables estudiadas o incluso, nos dará la oportunidad de hacer predicciones sobre futuras observaciones.

La primera opción es la más fácil. Consiste en estudiar si los puntos $(x_i,y_i)_{i=1,...,n}$ satisfacen una relación lineal de la forma

$$y = ax+b$$

con $a,b  \ \in \ R$.

En este caso, se busca la recta $y=ax+b$ que mejor aproxime los puntos dados imponiendo que la suma de los cuadrados de las diferencias entre los valores $y_i$ y sus aproximaiones $\tilde{y}_i=ax_i+b$ sea minima. Es decir, que:

$$\sum^n_{i=1}(y_i-\tilde{y}_i)^2$$
sea minima.

El objetivo de este tema no es otro más que enseñaros como hacer uso de $R$ para obtener esta recta de regresión.

Veremos también como se puede evaluar numericamente si esta recta se ajusta bien a las observaciones dadas.

Para ello, introduciremos algunas funciones de `R` y haremos uso de transformaciones logaritmicas para tratar casos en los que los puntos dados se aproximen mejor mediante una función exponencial o potencial.

## Como calcular una recta de regresión lineal

#### 1 Planteamiento del problema

Como ya hemos dicho, el objetivo de este tema es estudiar si existe relación lineal entre las variables dependientes e independiente.

Por lo general, cuando tenemos una serie de observaciones emparejadas, $(x_i,y_i)_{i=1,...,n}$ la forma natural de almacenarlas en `R` es mediante una tabla de datos. Y la que más conocemos nosotros es el data frame.

Como recordareis de temas anteriores, la ventaja de trabajar con este tipo de organización de datos es que luego se pueden hacer muchas cosas.

###### Ejemplo 1
En este ejemplo, nosotros haremos uso del siguiente data frame:

```{r Ejemplo 1 Regresión lineal}
# Llamando el fichero desde la web.
library(RCurl)
df <- getURL("https://raw.githubusercontent.com/argeus47/r-basic/master/data/bodyfat.txt", ssl.verifypeer = FALSE)

body <- read.csv(textConnection(df), 
      header = T, #Para que los nombres de la                                    cabecera aparescan
      sep = "", #Indica las separaciones de una columna con otra en el fichero, da error si el separador fue introducido equivocadamente
      dec = ".") #Puedo quitar el separador decimal 

head(body,3)
```

Más concretamente, trabajaremos con las variables `fat` y `weight`

```{r Ejemplo 1 definiendo variables para trabajar}
body2 = body[, #Todas las filas
             c(2,4)] #Columna 2 y 4 que es fat = grasa, weight = peso

names(body2) = c('Grasa', 'Peso') #Defino los nombres de la variable
str(body2)

head(body2,3)

```

#### 2 Representación gráfica

Al analizar datos, siempre es recomendable empezar con una representación grafica que nos permita hacernos a la idea de lo que tenemos.

Esto se consigue haciendo uso de la función `plot`, que ya hemos estudiado en detalle lecciones anteriores. No obstante, para lo que ncesitamos en este tema nos conformamos con un gráfico básico de estos puntos que nos muestre su distribución.

```{r Ejemplo 1 Creando grafico basico}
plot(body2)
```

#### 3 Calculando la recta de regresión

Para calcular la recta de regresión con `R` de la familia de puntos $(x_i,y_i)_{i=1,...,n}$, si `x` es el vector $(x_i)_{i=1,...,n}$ e `y` es el vector $(y_i)_{i=1,...,n}$, entonces, su recta de regresión se calcula mediante la instrucción

$$lm(y\sim x)$$

Cuidado con la sintaxis: primero va el vecto de las variables dependientes y, seguidamente depues de una tilde $\sim$, va el vector de las variables independientes.

Esto se debe a que `R` toma el significado de la tilde como `en función de`. Es decir, la interpretación de $lm(y\sim x)$ en `R` es `la recta de regresión de (y) en función de (x)`

Si los vectores `y` y `x` son, en este orden, la primera y la segunda columna de un data frame de dos variables, entonces es suficiente aplicar la función `lm` al data frame.

En general, si `x` e `y` son dos variables de un data frame, para calcular la recta de regresión de `y` en función de `x` podemos usar la instrucción

$$lm(y \sim x, data = dataframe)$$

```{r Ejemplo 1 usando lm}
#La recta de regresión peso en función del peso

lm(body2$Peso~body2$Grasa) #Opción 1

#Opción 2
lm(Peso~Grasa, #Peso en función de la grasa
   data = body2) #Datos para aplicar la función de regresión lineal

```
*Leyendo la información*
Para estimar el peso de un individuo hay que multiplicar la grasa 2.151 y sumarle al resultado 137.738

Como podeis observar, las dos formas de llamar la función dan exactamente lo mismo. Ninguna es mejor que la otra.

El resultado obtenido en ambos casos significa que la recta de regresión para nuestros datos es:

* `x` = Es el porcentaje de grasa corporal, se sustituye y se obtiene `y` que es porcentaje de peso

$$y=2.151x+137.738$$
Ahora podemos superponer esta recta a nuestro gráfico anterior haciendo uso de la función `abline()`

```{r Ejemplo 1 linea de la regresión lineal}

plot(body2)#Grafica

abline(lm(Peso~Grasa, #Peso en función de la grasa
          data = body2),#Datos para aplicar la función de regresión lineal)
       col = 'purple') #Color de la linea que pasa por el conjunto de datos 

```

###### Observación

Hay que tener en cuenta que el análisis llevado a cabo hasta el momento de los pares de valores $(x_i,Y_i)_{i=1,...,n}$ ha sido puramente descriptivo.

Es decir, hemos mostrado que estos datos son consistentes con una función lineal, pero no hemos demostrado que la variable dependiente sea función aproximadamente lineal de la variable dependiente. Esto ultimo necesitara una demostración matemática o bien un argumento biologico, pero no basta con una simple comprobación numérica.

###### Haciendo predicciones

Eso sí, podemos utilizar todo lo hecho hasta ahora par predecir valores $\tilde{y}_i$ en función de los $x_i$ resolviendo una simple ecuación lineal.

## Coeficiente de determinación (Investigar, cursos de machine learning)

El coeficiente de determinación, $R^2$, no s es util para evaluar numericamente si la relación lineal obtenida es significativa o no.

No explicaremos de momento como se define. Eso lo dejamos para curiosidad del usuario. Por el momento, es suficiente con saber que este coeficiente se encuentra en el intervalo [0,1]. Si $R^2$ es mayor a `0.9`, consideraremos que el ajuste es bueno. De lo contrario, no.


### La función summary

La función `summary` aplicada a `lm` nos muestra los contenidos de este objeto. Entre ellos encontramos `Multiple R-squared`, que no es ni más ni menos que el coeficiente de terminación, $R^2$.

Para facilitarnos las cosas y ahorrarnos información que, de momento, no nos resulta de interés, podemos aplicar `summary(lm(...))$r.squared`

```{r Ejemplo 1 aplicando función summary}

summary(
        lm(Peso~Grasa, #Peso en función de la grasa
           data = body2),#Datos para aplicar la función de regresión lineal)
)

```

```{r Ejemplo 1 determinando estrayendo el r square de summary}

summary(
        lm(Peso~Grasa, #Peso en función de la grasa
           data = body2),#Datos para aplicar la función de regresión lineal)
       )$r.square

```

* En este caso, hemos obtenido un coeficiente de determinación de 0.3751, *cosa que confirma que la recta de regresión no aproxima nada bien nuestros datos.* Esto quiere decir que nuestro modelo de información obtenida *NO ES LINEAL*


# Transformaciones logaritmicas de los datos

#### Transformaciones logaritmicas

No siempre encontraremos dependencias lineales. A veces nos encontraremos otro tipod e dependencias, como por ejemplo potencias o exponenciales.

Estas se pueden transformar a lineales mediante un cambio de escala.

#### Escalas logarítmicas

Por lo general, es habitual encontrarnos gráficos con sus ejes en escala lineal.

Es decir, las marcas en los ejes están igualmente espaciadas.

A veces, es conveniente dibujar alguno de los ejes en escala logaritmica, de modo que la misma distancia entre las marcas significa el mismo coeficiente entre sus valores. En otras palabras, un eje en escala logaritmica representa el logaritmo de sus valores en escala lineal.

Diremos que un gráfico está en escala semilogaritmica cuando su eje de abcisas esta en escala lineal y, el de ordenadas, en escala logartmica.

Diremos que un gráfico esta en escala doble logaritmica cuando ambos ejes estan en escala logaritmica.

#### Interpretación gráfica

* *ley aproximadamente exponencial*
Si al representar unos puntos $(x_i,Y_i)_{i=1,...,n}$ en escala semilogaritmica observamos que siguen aproximadamente una recta, esto querra decir que los valores $log(y)$ siguen una ley aproximadamente lineal en los valores `x` y, por lo tanto, que `y` sigue una **ley aproximadamente exponencial** en `x`

En efecto, si $log(y)=ax+b$, entonces,

$$y = 10^{log(y)} = 10{ax+b} = 10^{ax} \cdot 10^b = a^x \beta$$

con $\alpha = 10^{\alpha}$ y $\beta=10^b$

* *ley aprimadamente potencial*
Si al representar unos puntos $(x_i,Y_i)_{i=1,...,n}$ en escala doble logaritmica observamos que siguen aproximadamente una recta, esto querra decir que los valores $log(y)$ siguen una ley aproximadamente lineal en los valores $log(x)$, y, por lo tanto, que `y` sigue una **ley aprimadamente potencial** en `x`

En efecto, si $log(y) = a \ log(x)+b$, entonces, por propiedades de logaritmos

$$y = 10^{log(y)} = 10^{a \ log(x)+b} = (10^{log(x)})^a \cdot 10^b = x^{\alpha} \beta$$

con $\beta = 10^b$

## Ejecicios: relaciones exponenciales

#### Ejemplo 2

En este caso trabajaremos no con un data frame, si no directamente con los dos vectores siguientes:

```{r Ejemplo 2 Relaciones exponenciales}

dep = c(1.2, 3.6, 12, 36)
ind = c(20, 35, 61, 82)

par(mfrow = c(1,2)) ## Instrucción para poner 2 graficos en la misma linea  donde tiene 1 fila y 2 columnas de graficos 

#Grafica 1
plot(ind, dep, #Datos a graficar en x e y
     main = 'Escala lineal')

#Grafica 2
plot(ind, dep, #Datos a graficar en x e y
     log = 'y', #Invoca para graficar una función logaritmica en y
     main = 'Escala semilogarimica')

par(mfrow = c(1,1))  ##Siempre colocar esa función al final
```

```{r Ejemplo 2 aplicando modelo lineal}
#la variable (log) en función de la variable (ind)
lm(log10(dep)~ind)

summary(lm(log10(dep)~ind))$r.square #Determino el R^2 



```

Lo que acabamos de obtener es que 

$$log(dep) = 0.023 \ \cdot \ ind -0.33$$

es una buena aproximación de nuestros datos.

Con lo cual

$$dep = 10^{0.023 \ \cdot \ ind} \ \cdot \ 10^{-0.33}=1.054^{ind} \ \cdot \ 0.468$$
*(Matematicamente al despejar el log(dep) se eleva todo lo que esta despues de la igual es decir: dep=10^(valores)...)*

```{r Ejemplo 2 Graficando}

plot(ind,dep, #Valores originales
     main = 'Curva de regresión')
curve(1.054^x*0.468, #Valores obtenidos del calculo anterior
      add = T, #Para adicionar la curva al grafico
      col = 'purple')
```

## Ejercicios: relaciones potenciales

#### Ejemplo 3

En este caso trabajaremos con el siguiente data frame:

```{r Ejemplo 3 Relaciones potenciales }

tiempo = 1:10 #Variable independiente
gramos = c(0.097,0.709,2.698,6.928,15.242,29.944,52.982,83.983,120.612,161.711) #Variable dependiente
d.f = data.frame(tiempo,gramos) #Creo el data frame

par(mfrow = c(1,3)) ## Instrucción para poner 2 graficos en la misma linea  donde tiene 1 fila y 3 columnas de grafic
plot(d.f)
plot(d.f, log = 'y')
plot(d.f, log = 'xy')

par(mfrow = c(1,1))  ##Siempre colocar esa función al final
```

```{r Ejemplo 3 Aplicando modelo lineal}
#Log(gramos) en función de log(tiempo)
lm(log10(gramos)~log10(tiempo),
   data = d.f) #datos para estudio

summary(lm(log10(gramos)~log10(tiempo),data = d.f))$r.square #Determino el R^2

```


Lo que acabamos de obtener es que:

$$log(gramos) = 3.299*log*tiempo)-1.093$$
Es una buena aproximación de nuestros datos.

Con lo cual despejo:

$$gramos = 10^{3.299*log(tiempo)} * 10^{-1.093}$$

$$gramos = tiempo^{3.299} * 0.081$$
Grafico:
```{r Ejemplo 3 Graficando con la curva potencial}

#log(gramos) = 3.299*log*tiempo)-1.093
#Despejo
#gramos = 10^{3.299*log(tiempo)} * 10^{-1.093}
#gramos = tiempo^{3.299} * 0.081
a = 3.299
b = 10^(-1.093)
plot(d.f, #Valores originales
     main = 'Curva de regresión')

#Por ser potencia se hace x^(valor )
curve(x^(a)*b, #Valores obtenidos del calculo anterior
      add = T, #Para adicionar la curva al grafico
      col = 'purple')

```






